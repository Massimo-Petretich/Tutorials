---
author: "Massimo Petretich"
date: "March 2020"
title: "Bayesian Models and Regression"
output:
  html_document:
    toc: true
    number_sections: true
---



    
# Introduction 
## Bayesian vs Frequentist approach
A good introduction to the topic is presented by Annette Dobson in her book 'An introuction to generalized linear model'. Although the book itself does not focus primarily on bayesian models, it provides all the fundations required to build a bayesian thinking.<br>
When it comes to estimating the parameter(s) of a population, frequentist and bayesian approaches start from comletely different assumptions.<br>
In the frequentist approach the parameter is considered fixed whereas the random process used for its determination (observations) is variable (repeated 'n' times). The estimate of the 'true' parameter is achieved for n --> infinity. The estimate of the parameter ($\mathrm{P}(X \mid\theta)$) is obtained by finding the maximum point in the likelihood function.<br>
In the bayesian approach instead, the observations are considered fixed and the parameter is the variable entity. The parameter in bayesian statistics is considered just like any random variable, for example like 1 dice if we are throwing 2 dices, and, as a consequence its distribution can be modeled as a probability density function (or a probability mass function if we deal with count or categorical samples). In bayesian statistics  the probability density function of the parameter  ($\mathrm{P}(\theta)$) is referred to as 'prior' and represents our prior belief regarding the distribution of the parameter. The posterior distribution ($\mathrm{P}(\theta\mid X)$) is nothing but the product of the likelihood function and the prior distribution as shown in equation 1. The denominator in equation 1 is just the integral of this product over the whole parameter space. Its presence in the equation ensures that the posterior probability density function integrates to 1. THis terms is often omitted (in this tutorial it is omitted) for computational reasons since by doing so the estimates of the parameter's value does not change.<br>
<br>
Equation 1: <br>
$\mathrm{P}(\theta\mid X) = \frac {\mathrm{P}(X \mid\theta)\mathrm{P}(\theta)} {\sum \mathrm{P}(X \mid\theta)\mathrm{P}(\theta)}$ <br>

## Markov-Chain Monte-Carlo
While the maximum likelihood algorithms aim to find the maximum point of the likelihood in the most efficient way (with less computations), in the bayesian approach, the aim is to 'map' (or in more statistical terms, "sample") the posterior probability density function as finely as possible in order to estimate not only the maximum value, but also, the confidence intervals around this value. This may not be necessary for very simple models (would be just an inefficient way of determining the parameter's value). This approach gains power in all those situation where the likelihood functions may be too complex to be determined algebrically, may be unknown, or have multi-modal nature (in this latter case the maximum likelihood methods would just find 1 maximum value, for more details, consult the Apendix). There are different algorithms available to map the parameter space of the posterior probability density function. Here we use the classic Markov-Chain Monte-Carlo (MCMC) Metropolis-Hastings algorithm. Despite the fact that several physicists contributed to the development of the Monte-Carlo Markov-Chain method, it was Stanislaw Ulam who implemented the modern version of the method in the '40s. The name Monte-Carlo is alleged to originate from the place where a relative from Stan Ulam used to gamble. Despite the fact that the explanation of the Metropolis-Hastings algorithm goes beyond the scope of this tutorial, the example below introduces its logic with a simple practical application. <br><br><br>

# The Metropolis-Hastings algorithm
## Normal distribution example
In this example we use the Metropolis-Hastings algorithm to sample the probability density function of a normal distribution with $\mu = 10$ and $\sigma  = 2$. <br><br>
Equation2: Probability density function of the normal distribution <br>
$\mathrm{P}(X \mid \mu, \sigma^{2}) = (2 \pi \sigma^{2})^{-1/2} e^{-0.5(\frac{X-\mu}{\sigma})}$ <br><br>
Importantly, here we are not yet applying any bayesian statistical concept. <br> <br>

1: MONTE-CARLO part of the algorithm (random sampling). Generating a new random variable <br>
2: MARKOV-CHAIN part of the algorithm, the improvement is calculated on the basis of the previous value (compares the probabilities of the new random variable with the one of the previous iteration)<br>
3: If the new random variable gives an improvement (> 1) this line returns 1 <br>
4: This step ensures that the algorithm does not get stuck on the point of maxima (like it would in a standard maximum likelihood estimate) but keeps refining the mapping around the point of maxima<br>

```{r message=FALSE}
library(tidyverse)
library(ggplot2)

model_pdf <- function(x, mu, sigma) exp(-0.5 * ((x - mu) / sigma)^2) / (sigma*sqrt(2*pi)) # probability density function
proposed <- function(x, sd) rnorm(n = 1, mean = x, sd = sd) # random variable generation

iterations = 1e4 # number of iterations
x <- -20 # starting value of the chain
sigma = 2
mu = 10
stored <- rep(NA, iterations)
improvement <- rep(NA, iterations)

for (i in 1:iterations) {
  x_i <- proposed(x, sigma) # 1
  improvement[i] <- model_pdf(x_i, mu, sigma) / model_pdf(x, mu, sigma) # 2
  ratio <- min(1, improvement[i]) # 3
  accept <- runif(1) < ratio # 4
  stored[i] <- ifelse(accept, x_i, x)
  x <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=sample)) + geom_histogram(bins = 100)
ggplot(data = results_combined, aes(x = iteration, y = improvement)) + geom_line(colour = 'grey50') + scale_y_log10()
```



# Bayesian model fitting
## Normal distribution (linear model with just the intercept as parameter)

In this example we introduce the concept of bayesian statistics by plugging into the Metropolis-Hastings algorithm a new probability density function: the posterior from the bayesian equation (instead of the simple normal distribution probability density function). As mentioned in the introduction in this implementation we omit the denominator (equation 1).

```{r message=FALSE}
library(tidyverse)
library(ggplot2)

X <- rnorm(n = 20, mean = 10, sd = 4) # Our sample from the population (normal population with mean 10 and standard deviation 4)

qplot(x = 1:20, y = X) + expand_limits(y = c(-10, 30))

model_pdf <-  function(obs, mu, sigma) exp(-0.5*((obs-mu)/sigma)^2) / (sqrt(sigma^2*2*pi))

posterior_pdf <- function(x, mu, sigma, mu_prior, sigma_prior) model_pdf(x, mu, sigma) * model_pdf(mu, mu_prior, sigma_prior) # likelihood * prior

proposed <- function(mu, sd) rnorm(n = 1, mean = mu, sd = sd)

iterations = 1e4
mu <- 0 # start of the chain (slope)
sigma_proposed = 0.5 

sigma <- 5
mu_prior <- 7
sigma_prior <- 10 # standard deviation of the normal model used for the likelihood probability density function. Here we consider it as a nuisance parameter, namely, we do not aim to estimate it but we provide it to the model 
stored <- rep(NA, iterations)


for (i in 1:iterations) {
  mu_prime <- proposed(mu, sigma_proposed)
  
  i_posterior_p <- posterior_pdf(X, mu_prime, sigma, mu_prior, sigma_prior)
  i_posterior_p <- Reduce(`*`, i_posterior_p)
  
  previous_posterior_p <- posterior_pdf(X, mu, sigma, mu_prior, sigma_prior)
  previous_posterior_p <- Reduce(`*`, previous_posterior_p)
  
  improvement <- i_posterior_p / previous_posterior_p
  
  ratio <- min(1, improvement)
  accept <- runif(1) < ratio
  stored[i] <- ifelse(accept, mu_prime, mu)
  mu <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=sample)) + geom_histogram(bins = 100)
```



## Linear model (parameter: slope)
The previous example was the simplest linear model, the one with only the intercept as parameter. <br>
In the next example we will implement the next step: estimating the slope ('b') of a linear model (for now) without intercept. <br>
Our prior distribution (regarding the estimate of the coefficient 'b') is 2.5 with a standard deviation of 0.5. To be noted in the results that with this 'strong' (small sigma) prior, the estimate of the posterior probability density function is pulled towards 2.5 (mean of the prior) when in reality it is 3 (in the dataset used). Excersise: try to weaken the prior by increasing sigma and see if the estimate of the coefficient 'b' approaches 3.

```{r message=FALSE}
library(tidyverse)
library(ggplot2)

population_b <- 3 # slope
dataset <- data.frame(x = 1:20, y = sapply(1:20, function(x) population_b * (x + rnorm(n = 1, mean = 1, sd = 1))))
ggplot(data = dataset, aes(x = x, y = y)) + geom_point(colour = 'grey50')

model_pdf <-  function(obs, mu, sigma) exp(-0.5*((obs-mu)/sigma)^2)/(sqrt(sigma^2*2*pi))

posterior_pdf <- function(y, x, b, sigma, b_prior, sigma_prior) model_pdf(y, b*x, sigma) * model_pdf(b, b_prior, sigma_prior) # likelihood * prior

proposed <- function(b, sd) rnorm(n = 1, mean = b, sd = sd)

iterations = 1e4
b <- 0 # start of the chain (slope)
sigma_proposed = 0.2 
sigma <- 10 # standard deviation of the normal model used for the likelihood probability density function. Here we consider it as a nuisance parameter, namely, we do not aim to estimate it but we provide it to the model 
b_prior <- 2.5 # prior belief of the slope 
sigma_prior <- 0.5 # standard deviation of the coefficient, not y!
stored <- rep(NA, iterations)


for (i in 1:iterations) {
  b_prime <- proposed(b, sigma_proposed)
  
  i_posterior_p <- posterior_pdf(dataset$y, dataset$x, b_prime, sigma, b_prior, sigma_prior)
  i_posterior_p <- Reduce(`*`, i_posterior_p)
  
  previous_posterior_p <- posterior_pdf(dataset$y, dataset$x, b, sigma, b_prior, sigma_prior)
  previous_posterior_p <- Reduce(`*`, previous_posterior_p)
  
  improvement <- i_posterior_p / previous_posterior_p
  
  ratio <- min(1, improvement)
  accept <- runif(1) < ratio
  stored[i] <- ifelse(accept, b_prime, b)
  b <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=sample)) + geom_histogram(bins = 100)
```

## Linear model (2 parameters: intercept, slope)

Let's put together the previous 2 examples

```{r message=FALSE}
library(tidyverse)
library(ggplot2)

population_b0 <- 20 # intercept
population_b1 <- 3 # slope

dataset <- data.frame(x = 1:20, 
                  y = sapply(1:20, function(x) population_b0 + population_b1 * (x+rnorm(n = 1, mean = 1, sd = 1)))
                  )
ggplot(data = dataset, aes(x = x, y = y)) + geom_point(colour = 'grey50') + expand_limits(y = 0)

model_pdf <-  function(obs, mu, sigma) exp(-0.5*((obs-mu)/sigma)^2) / (sqrt(sigma^2*2*pi))

posterior_pdf <- function(y, x, b0, b1, sigma, b0_prior, b1_prior, sigma0_prior, sigma1_prior) {
    log( model_pdf(y, b0 + b1 * x, sigma) ) + 
    log( model_pdf(b0, b0_prior, sigma0_prior) ) +
    log( model_pdf(b1, b1_prior, sigma1_prior) )
} # conversely to the example above, here we must take the log of the probabilities, otherwise with 3 probability products the results will be most likely always 0 

proposed <- function(b, sd) rnorm(n = 1, mean = b, sd = sd)

iterations = 1e4
b0 <- 10 # start of the chain (intercept)
b1 <- 1 # start of the chain (slope)
sigma_proposed_b0 = 2
sigma_proposed_b1 = 0.5

sigma <- 10 # standard deviation of the normal model used for the likelihood probability density function. Also in this case we provide it to the model and we do not estimate it
b0_prior <- 25 # prior belief of the intercept
b1_prior <- 2.5 # prior belief of the slope
sigma0_prior <- 10 # standard deviation of the intercept
sigma1_prior <- 2.5 # standard deviation of the slope

coef_b0 <- rep(NA, iterations)
coef_b1 <- rep(NA, iterations)

for (i in 1:iterations) {
  b0_prime <- proposed(b0, sigma_proposed_b0)
  b1_prime <- proposed(b1, sigma_proposed_b1)
  
  i_posterior_p <- posterior_pdf(dataset$y, dataset$x, b0_prime, b1_prime, sigma, b0_prior, b1_prior, sigma0_prior, sigma1_prior)
  i_posterior_p <- Reduce(`+`, i_posterior_p)
  
  previous_posterior_p <- posterior_pdf(dataset$y, dataset$x, b0, b1, sigma, b0_prior, b1_prior, sigma0_prior, sigma1_prior)
  previous_posterior_p <- Reduce(`+`, previous_posterior_p)
  
  improvement <- exp(i_posterior_p - previous_posterior_p)
  
  ratio <- min(1, improvement)
  
  accept <- runif(1) < ratio

  coef_b0[i] <- ifelse(accept, b0_prime, b0)
  coef_b1[i] <- ifelse(accept, b1_prime, b1)
  
  b0 <- coef_b0[i]
  b1 <- coef_b1[i]
}

results_combined = data.frame(b0 = coef_b0, b1 = coef_b1, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = b0)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=b0)) + geom_histogram(bins = 100)

ggplot(data = results_combined, aes(x = iteration, y = b1)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=b1)) + geom_histogram(bins = 100)

ggplot(data = results_combined, aes(x = b0, y = b1)) + geom_line(colour = 'grey50')

```

Let's fit the full linear model

```{r message=FALSE}
b0 <- median(results_combined$b0)
b1 <- median(results_combined$b1)

ggplot(data = dataset, aes(x = x, y = y)) + geom_point(colour = 'grey50') + geom_abline(intercept = b0, slope = b1) + expand_limits(y = 0)
```

And check the residuals

```{r message=FALSE}
dataset$residuals <- dataset$y - (b0 + b1 * dataset$x)

ggplot(data = dataset, aes(x = x, y = residuals)) + geom_point(colour = 'grey50') + expand_limits(y = c(-20, 20))
```

As shown by the residuals, the model is good although not perfect. The relatively strong prior is slightly 'forcing' the parameter's estimate.



# Apendix: Potential advantages of the bayesian approach over the frequentist one
## Bimodal normal distributions example
In this example we will introduce a situation where a bayesian approach might have some advantages over maximum likelihood. In this example we will map the probability density function of a more complex distribution, namely one containing 2 normal distributions.<br>

```{r message=FALSE}
library(tidyverse)
library(ggplot2)


model_pdf <- function(x, sigma) {
  exp(-0.5*((x-mu1)/sigma)^2) / (sigma*sqrt(2*sigma*pi)) + 
  exp(-0.5*((x-mu2)/sigma)^2) / (sigma*sqrt(2*sigma*pi)) 
}

proposed <- function(x, sd) rnorm(n = 1, mean = x, sd = sd)

iterations = 1e4 # number of iterations
x <- -20 # starting value of the chain
sigma = 2 
sigma_proposed = 10
mu1 = 10
mu2 = 40
stored <- rep(NA, iterations)
improvement <- rep(NA, iterations)


for (i in 1:iterations) {
  x_i <- proposed(x, sigma_proposed)
  improvement[i] <- model_pdf(x_i, sigma) / model_pdf(x,  sigma)
  ratio <- min(1, improvement[i])
  accept <- runif(1) < ratio
  stored[i] <- ifelse(accept, x_i, x)
  x <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=sample)) + geom_histogram(bins = 100)
ggplot(data = results_combined, aes(x = iteration, y = improvement)) + geom_line(colour = 'grey50') + scale_y_log10()
```

An important consideration regarding situations like the one in this example, is that often MCMC algorithms do not manage to sample both distributions (may get stuck on 1 of the 2). This can happen for example if we used a lower sigma_proposed, in that case, what happens is that if the random steps are not large enough to "jump" on high probability regions on the other distributions, the algorith remains stuck on just 1 distribution. Let's run the same example above but with sigma_proposed of 3 instead of 10.  

```{r message=FALSE, echo = FALSE}
library(tidyverse)
library(ggplot2)


model_pdf <- function(x, sigma) {
  exp(-0.5*((x-mu1)/sigma)^2) / (sigma*sqrt(2*sigma*pi)) + 
  exp(-0.5*((x-mu2)/sigma)^2) / (sigma*sqrt(2*sigma*pi)) 
}

proposed <- function(x, sd) rnorm(n = 1, mean = x, sd = sd)

iterations = 1e4 # number of iterations
x <- -20 # starting value of the chain
sigma = 2 
sigma_proposed = 3
mu1 = 10
mu2 = 40
stored <- rep(NA, iterations)
improvement <- rep(NA, iterations)


for (i in 1:iterations) {
  x_i <- proposed(x, sigma_proposed)
  improvement[i] <- model_pdf(x_i, sigma) / model_pdf(x,  sigma)
  ratio <- min(1, improvement[i])
  accept <- runif(1) < ratio
  stored[i] <- ifelse(accept, x_i, x)
  x <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
```

Note that starting the chain from -20 allow us to sample only from the mu1 distribution (mean = 10) and we completely miss the mu2 distribution (mean = 40). <br>
The way this problem was solved in the implementation above was simply to inflate the sigma_proposed parameter to allow the sampler to perform bigger jumps. <br> 
Normally, more sophisticated MCMC algorithms (compared to the very simple one shown here) use a different strategy to solve this problem, namely, run different MCMC chains starting from different values and then combine the results. <br><br>
Let's try to run the example above starting the chain from 60 instead of -20. In this case we sample uniquely from the mu2 distribution (mean = 40).


```{r message=FALSE, echo = FALSE}
library(tidyverse)
library(ggplot2)


model_pdf <- function(x, sigma) {
  exp(-0.5*((x-mu1)/sigma)^2) / (sigma*sqrt(2*sigma*pi)) + 
  exp(-0.5*((x-mu2)/sigma)^2) / (sigma*sqrt(2*sigma*pi)) 
}

proposed <- function(x, sd) rnorm(n = 1, mean = x, sd = sd)

iterations = 1e4 # number of iterations
x <- 60 # starting value of the chain
sigma = 2 
sigma_proposed = 3
mu1 = 10
mu2 = 40
stored <- rep(NA, iterations)
improvement <- rep(NA, iterations)


for (i in 1:iterations) {
  x_i <- proposed(x, sigma_proposed)
  improvement[i] <- model_pdf(x_i, sigma) / model_pdf(x,  sigma)
  ratio <- min(1, improvement[i])
  accept <- runif(1) < ratio
  stored[i] <- ifelse(accept, x_i, x)
  x <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
```


## Hipothesis testing
Before we mentioned that a bayesian approach might be more accurate than a frequentist one in cases such the one where we have a multi-modal probability density function.
There is an another scenario where the frequentist approach would be misleading. The frequentist approach tend to be sensitive to the H0 (null hypothesis): it can happen that a population's parameter is  truly different from the H0 value, however, whenever the sample size is low, or the standard deviation is high, we might fail to reject H0. <br>
Let's take a look to the example below. Let's suppose that we are asked to bet if the sample 'X' has mean 9 or 10. With a frequentist approach we would use 1 of the 2 values (let's say 9) as H0 and test if the mean differs from it (here for the purpose of the example we know it does since we specify $\mu = 10$ when the random variable 'X' is generated). In this case, the p-value is 0.07, if we use a cutoff of 0.05 we would fail to reject H0 and therefore we would bet on the mean of the random variable to be 9.


```{r}
set.seed(1)
X <- rnorm(n = 7, mean = 10, sd = 1.5) # Our sample from the population (normal population with mean 10 and standard deviation 4)
X
t.test(x = X, mu = 9)
```


```{r message=FALSE}
library(tidyverse)
library(ggplot2)

model_pdf <-  function(obs, mu, sigma) exp(-0.5*((obs-mu)/sigma)^2) / (sqrt(sigma^2*2*pi))

posterior_pdf <- function(x, mu, sigma, mu_prior, sigma_prior) model_pdf(x, mu, sigma) * model_pdf(mu, mu_prior, sigma_prior) # likelihood * prior

proposed <- function(mu, sd) rnorm(n = 1, mean = mu, sd = sd)

iterations = 1e4
mu <- 0 # start of the chain (slope)
sigma_proposed = 0.5 

sigma <- 2
mu_prior <- 9.5 # we pick an uninformative (unbiased) prior, which is exactly in the middle of the 2 values of interest
sigma_prior <- 10
stored <- rep(NA, iterations)


for (i in 1:iterations) {
  mu_prime <- proposed(mu, sigma_proposed)
  
  i_posterior_p <- posterior_pdf(X, mu_prime, sigma, mu_prior, sigma_prior)
  i_posterior_p <- Reduce(`*`, i_posterior_p)
  
  previous_posterior_p <- posterior_pdf(X, mu, sigma, mu_prior, sigma_prior)
  previous_posterior_p <- Reduce(`*`, previous_posterior_p)
  
  improvement <- i_posterior_p / previous_posterior_p
  
  ratio <- min(1, improvement)
  accept <- runif(1) < ratio
  stored[i] <- ifelse(accept, mu_prime, mu)
  mu <- stored[i]
}

results_combined = data.frame(sample = stored, improvement = improvement, iteration = 1:iterations)

ggplot(data = results_combined, aes(x = iteration, y = sample)) + geom_line(colour = 'grey50')
ggplot(data = results_combined, aes(y=sample)) + geom_histogram(bins = 100)
```


```{r}
median(results_combined$sample)
```

Estimate of mu using the bayesian approach would make us choose correctly: $\mu = 10$. This happens for the reason explained above, the frequentist approach is very sensitive to the H0 and needs solid evidences in order to reject it.

# Bayesian statistics in R
Packages: rstan, BAS (recmmended) <br>
Additional: atmcmc, BRugs, mcmc, MCMCpack, ramcmc.